# Chain-of-Thought for Wireless Communications

## References of different CoT technologies and their applications in wireless communications and networking

[A1] Wang L, Xu W, Lan Y, et al. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091, 2023.
[A2] Shaikh O, Zhang H, Held W, et al. On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning. arXiv preprint arXiv:2212.08061, 2022.
[A3] Chen W. Large language models are few (1)-shot table reasoners. arXiv preprint arXiv:2210.06710, 2022.
[A4] Li J, Li G, Li Y, et al. Structured chain-of-thought prompting for code generation. ACM Transactions on Software Engineering and Methodology, 2025, 34(2): 1-23.
[A5] Shum K S, Diao S, Zhang T. Automatic prompt augmentation and selection with chain-of-thought from labeled data. arXiv preprint arXiv:2302.12822, 2023.
[A6] Williams C Y K, Miao B Y, Kornblith A E, et al. Evaluating the use of large language models to provide clinical recommendations in the Emergency Department. Nature Communications, 2024, 15(1): 8236.
[A7] Chen X, Aksitov R, Alon U, et al. Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311, 2023.
[A8] Singh K, Bhowmick S, Moturi P, et al. NO STRESS NO GAIN: STRESS TESTING BASED SELF-CONSISTENCY FOR OLYMPIAD PROGRAMMING//ICLR 2025 Workshop: VerifAI: AI Verification in the Wild.
[A9] Long J. Large language model guided tree-of-thought[J]. arXiv preprint arXiv:2305.08291, 2023.
[A10] Lei B, Liao C, Ding C. Boosting logical reasoning in large language models through a new framework: The graph of thought. arXiv preprint arXiv:2308.08614, 2023.
[B1] Tong J, Shao J, Wu Q, et al. Wirelessagent: Large language model agents for intelligent wireless networks. arXiv preprint arXiv:2409.07964, 2024.
[B2] Xu M, Niyato D, Zhang H, et al. Cached model-as-a-resource: Provisioning large language model agents for edge intelligence in space-air-ground integrated networks. arXiv preprint arXiv:2403.05826, 2024.
[B3] Zhou L, Deng X, Wang Z, et al. Semantic information extraction and multi-agent communication optimization based on generative pre-trained transformer. IEEE Transactions on Cognitive Communications and Networking, 2024.
[B4] Xu M, Niyato D, Kang J, et al. When large language model agents meet 6G networks: Perception, grounding, and alignment. IEEE Wireless Communications, 2024.
